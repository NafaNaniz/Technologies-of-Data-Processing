{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Введение в обработку текста на естественном языке"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Материалы:\n",
    "* Макрушин С.В. Лекция 9: Введение в обработку текста на естественном языке\\\n",
    "* https://realpython.com/nltk-nlp-python/\n",
    "* https://scikit-learn.org/stable/modules/feature_extraction.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Задачи для совместного разбора"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Лабораторная работа 9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Расстояние редактирования"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.1 Загрузите предобработанные описания рецептов из файла `preprocessed_descriptions.csv`. Получите набор уникальных слов `words`, содержащихся в текстах описаний рецептов (воспользуйтесь `word_tokenize` из `nltk`). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['string',\n",
       " 'can',\n",
       " 'the',\n",
       " 'substrings',\n",
       " 'tokenizers',\n",
       " 'divide',\n",
       " 'in',\n",
       " 'example',\n",
       " 'and',\n",
       " 'Tokenizers',\n",
       " 'into',\n",
       " 'a',\n",
       " 'punctuation',\n",
       " 'be',\n",
       " 'strings',\n",
       " 'lists',\n",
       " 'words',\n",
       " '.',\n",
       " 'to',\n",
       " 'For',\n",
       " 'find',\n",
       " ',',\n",
       " 'of',\n",
       " 'used']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "text = 'Tokenizers divide strings into lists of substrings. For example, tokenizers can be used to find the words and punctuation in a string.'\n",
    "words = list(set(nltk.word_tokenize(text)))\n",
    "words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.2 Сгенерируйте 5 пар случайно выбранных слов и посчитайте между ними расстояние редактирования."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Пара: ['to', 'tokenizers'], расстояние редактирования: 8\n",
      "Пара: ['a', 'find'], расстояние редактирования: 4\n",
      "Пара: ['Tokenizers', 'words'], расстояние редактирования: 8\n",
      "Пара: ['used', ','], расстояние редактирования: 4\n",
      "Пара: [',', 'string'], расстояние редактирования: 6\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "from nltk.metrics import *\n",
    "lst = [0] * 5\n",
    "for i in range(5):\n",
    "    lst[i] = random.sample(words, 2)\n",
    "    print(f'Пара: {lst[i]}, расстояние редактирования: {edit_distance(lst[i][0], lst[i][1])}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.3 Напишите функцию, которая для заданного слова `word` возвращает `k` ближайших к нему слов из списка `words` (близость слов измеряется с помощью расстояния Левенштейна)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['divide', 'find', 'lists', 'in', 'and']\n"
     ]
    }
   ],
   "source": [
    "def find_words(word, k):\n",
    "    dct = {w: edit_distance(w, word) for w in words}\n",
    "    dct = sorted(dct, key = dct.get)[:k]\n",
    "    return dct\n",
    "print(find_words('divide', 5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Стемминг, лемматизация"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.1 На основе результатов 1.1 создайте `pd.DataFrame` со столбцами: \n",
    "    * word\n",
    "    * stemmed_word \n",
    "    * normalized_word \n",
    "\n",
    "Столбец `word` укажите в качестве индекса. \n",
    "\n",
    "Для стемминга воспользуйтесь `SnowballStemmer`, для нормализации слов - `WordNetLemmatizer`. Сравните результаты стемминга и лемматизации."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>stemmed_word</th>\n",
       "      <th>normalized_word</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>string</th>\n",
       "      <td>string</td>\n",
       "      <td>string</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>can</th>\n",
       "      <td>can</td>\n",
       "      <td>can</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>the</th>\n",
       "      <td>the</td>\n",
       "      <td>the</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>substrings</th>\n",
       "      <td>substr</td>\n",
       "      <td>substring</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tokenizers</th>\n",
       "      <td>token</td>\n",
       "      <td>tokenizers</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>divide</th>\n",
       "      <td>divid</td>\n",
       "      <td>divide</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>in</th>\n",
       "      <td>in</td>\n",
       "      <td>in</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>example</th>\n",
       "      <td>exampl</td>\n",
       "      <td>example</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>and</th>\n",
       "      <td>and</td>\n",
       "      <td>and</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Tokenizers</th>\n",
       "      <td>token</td>\n",
       "      <td>Tokenizers</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>into</th>\n",
       "      <td>into</td>\n",
       "      <td>into</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>a</th>\n",
       "      <td>a</td>\n",
       "      <td>a</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>punctuation</th>\n",
       "      <td>punctuat</td>\n",
       "      <td>punctuation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>be</th>\n",
       "      <td>be</td>\n",
       "      <td>be</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>strings</th>\n",
       "      <td>string</td>\n",
       "      <td>string</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lists</th>\n",
       "      <td>list</td>\n",
       "      <td>list</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>words</th>\n",
       "      <td>word</td>\n",
       "      <td>word</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>.</th>\n",
       "      <td>.</td>\n",
       "      <td>.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>to</th>\n",
       "      <td>to</td>\n",
       "      <td>to</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>For</th>\n",
       "      <td>for</td>\n",
       "      <td>For</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>find</th>\n",
       "      <td>find</td>\n",
       "      <td>find</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>,</th>\n",
       "      <td>,</td>\n",
       "      <td>,</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>of</th>\n",
       "      <td>of</td>\n",
       "      <td>of</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>used</th>\n",
       "      <td>use</td>\n",
       "      <td>used</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            stemmed_word normalized_word\n",
       "string            string          string\n",
       "can                  can             can\n",
       "the                  the             the\n",
       "substrings        substr       substring\n",
       "tokenizers         token      tokenizers\n",
       "divide             divid          divide\n",
       "in                    in              in\n",
       "example           exampl         example\n",
       "and                  and             and\n",
       "Tokenizers         token      Tokenizers\n",
       "into                into            into\n",
       "a                      a               a\n",
       "punctuation     punctuat     punctuation\n",
       "be                    be              be\n",
       "strings           string          string\n",
       "lists               list            list\n",
       "words               word            word\n",
       ".                      .               .\n",
       "to                    to              to\n",
       "For                  for             For\n",
       "find                find            find\n",
       ",                      ,               ,\n",
       "of                    of              of\n",
       "used                 use            used"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer \n",
    "from nltk.stem import SnowballStemmer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stemmer = SnowballStemmer(language='english')\n",
    "df = pd.DataFrame({'stemmed_word': [stemmer.stem(word) for word in words], 'normalized_word': [lemmatizer.lemmatize(word) for word in words]}, index=words)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.2. Удалите стоп-слова из описаний рецептов. Какую долю об общего количества слов составляли стоп-слова? Сравните топ-10 самых часто употребляемых слов до и после удаления стоп-слов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['tokenizers',\n",
       " 'divide',\n",
       " 'strings',\n",
       " 'lists',\n",
       " 'substrings',\n",
       " '.',\n",
       " 'example',\n",
       " ',',\n",
       " 'tokenizers',\n",
       " 'used',\n",
       " 'find',\n",
       " 'words',\n",
       " 'punctuation',\n",
       " 'string',\n",
       " '.']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "text = 'Tokenizers divide strings into lists of substrings. For example, tokenizers can be used to find the words and punctuation in a string.'\n",
    "tokens = word_tokenize(text.lower())\n",
    "english_stopwords = stopwords.words('english')\n",
    "tokens_without_stopwords = [word for word in tokens if word not in english_stopwords]\n",
    "tokens_without_stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Векторное представление текста"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3.1 Выберите случайным образом 5 рецептов из набора данных. Представьте описание каждого рецепта в виде числового вектора при помощи `TfidfVectorizer`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "Indexing with sparse matrices is not supported except boolean indexing where matrix and index are equal shapes.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[1;32mIn [55]\u001b[0m, in \u001b[0;36m<cell line: 8>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      7\u001b[0m k \u001b[38;5;241m=\u001b[39m vectors\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m vector \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(k):\n\u001b[1;32m----> 9\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[43mvectors\u001b[49m\u001b[43m[\u001b[49m\u001b[43mvectors\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241m.\u001b[39mtoarray())\n",
      "File \u001b[1;32mC:\\anaconda3\\lib\\site-packages\\scipy\\sparse\\_index.py:33\u001b[0m, in \u001b[0;36mIndexMixin.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m     32\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__getitem__\u001b[39m(\u001b[38;5;28mself\u001b[39m, key):\n\u001b[1;32m---> 33\u001b[0m     row, col \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_validate_indices\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     34\u001b[0m     \u001b[38;5;66;03m# Dispatch to specialized methods.\u001b[39;00m\n\u001b[0;32m     35\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(row, INT_TYPES):\n",
      "File \u001b[1;32mC:\\anaconda3\\lib\\site-packages\\scipy\\sparse\\_index.py:129\u001b[0m, in \u001b[0;36mIndexMixin._validate_indices\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m    127\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_validate_indices\u001b[39m(\u001b[38;5;28mself\u001b[39m, key):\n\u001b[0;32m    128\u001b[0m     M, N \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mshape\n\u001b[1;32m--> 129\u001b[0m     row, col \u001b[38;5;241m=\u001b[39m \u001b[43m_unpack_index\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    131\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m isintlike(row):\n\u001b[0;32m    132\u001b[0m         row \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(row)\n",
      "File \u001b[1;32mC:\\anaconda3\\lib\\site-packages\\scipy\\sparse\\_index.py:279\u001b[0m, in \u001b[0;36m_unpack_index\u001b[1;34m(index)\u001b[0m\n\u001b[0;32m    275\u001b[0m \u001b[38;5;66;03m# Next, check for validity and transform the index as needed.\u001b[39;00m\n\u001b[0;32m    276\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m isspmatrix(row) \u001b[38;5;129;01mor\u001b[39;00m isspmatrix(col):\n\u001b[0;32m    277\u001b[0m     \u001b[38;5;66;03m# Supporting sparse boolean indexing with both row and col does\u001b[39;00m\n\u001b[0;32m    278\u001b[0m     \u001b[38;5;66;03m# not work because spmatrix.ndim is always 2.\u001b[39;00m\n\u001b[1;32m--> 279\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mIndexError\u001b[39;00m(\n\u001b[0;32m    280\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mIndexing with sparse matrices is not supported \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m    281\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mexcept boolean indexing where matrix and index \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m    282\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mare equal shapes.\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m    283\u001b[0m bool_row \u001b[38;5;241m=\u001b[39m _compatible_boolean_index(row)\n\u001b[0;32m    284\u001b[0m bool_col \u001b[38;5;241m=\u001b[39m _compatible_boolean_index(col)\n",
      "\u001b[1;31mIndexError\u001b[0m: Indexing with sparse matrices is not supported except boolean indexing where matrix and index are equal shapes."
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "vectorizer = TfidfVectorizer()\n",
    "recipes = pd.read_csv('recipes_sample.csv', delimiter=',')\n",
    "recipes = recipes.head()\n",
    "vectors  = vectorizer.fit_transform(recipes['description'])\n",
    "# print(vectorizer.vocabulary_)\n",
    "k = vectors.shape[0]\n",
    "for vector in range(k):\n",
    "    print(vectors[vectors].toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3.2 Вычислите близость между каждой парой рецептов, выбранных в задании 3.1, используя косинусное расстояние (`scipy.spatial.distance.cosine`) Результаты оформите в виде таблицы `pd.DataFrame`. В качестве названий строк и столбцов используйте названия рецептов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9725198666999536\n",
      "0.8944228631111404\n",
      "0.8408701219963103\n",
      "0.8093525908759086\n",
      "1.0\n",
      "0.8551689750098787\n",
      "0.9214042717289879\n",
      "0.9640049285228507\n",
      "0.9529584291886545\n",
      "0.8788072331736545\n"
     ]
    }
   ],
   "source": [
    "from scipy.spatial import distance\n",
    "import numpy\n",
    "import pandas as pd\n",
    "lst =[]\n",
    "for el in recipes['description']:\n",
    "    lst.append(word_tokenize(el))\n",
    "vectors  = vectorizer.fit_transform(recipes['description']).toarray()\n",
    "for index_1 in range(len(lst)):\n",
    "    for index_2 in range(index_1 + 1, len(lst)):\n",
    "        print(distance.cosine(vectors[index_1], vectors[index_2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
